{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data read from csv file.\n",
      "Deleting columns containing huge missing values, Single valued items and id columns which are of no use.\n",
      "No data leakage is present as such.\n",
      "Changing the type of columns from object to category.\n",
      "Separating numeric data from the data set to perform numeric imputation on missing values.\n",
      "Imputed categorical data with the mode of the respective column.\n",
      "Imputed numeric columns to its median value.\n",
      "Setting aside 20% of the data as hold_out.\n",
      "Using standard scaler of sklearn library to Standardize and scale the numeric inputs.\n",
      "\n",
      "Finding Optimal parameters for 4 different models using GridSearch.\n",
      "Optimal Parameters for: \n",
      "1. Regularized Logistic Regression: {'model__alpha': 0.0004}\n",
      "2. Decision Tree: {'model__max_depth': 6, 'model__max_features': 30}\n",
      "3. Random Forest: {'model__n_estimators': 50, 'model__max_depth': 5, 'model__max_features': 5}\n",
      "4. Gradient Boosting Classifier: {'model__n_estimators': 70, 'model__max_depth': 2, 'model__max_features': 25}\n",
      "\n",
      "Performing Cross validation for 4 different models using optimal parameters obtained from GridSearch.\n",
      "Cross Validation Scores for: \n",
      "1. Regularized Logistic Regression: 0.739036\n",
      "2. Decision Tree: 0.765513\n",
      "3. Random Forest: 0.817901\n",
      "4. Gradient Boosting Classifier: 0.831696\n",
      "\n",
      "Based on the Cross validation ouptuts, the best model that can be selected is: 'Gradient Boosting Classifier'\n",
      "\n",
      "Fitting Gradient Boosting classifier for the dataset.\n",
      "Testing accuracy of this model on Test data(or hold_out data).\n",
      "\n",
      "Holdout ROC/AUC accuracy for Gradient Boosting classifier: 0.856075510586\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.options.mode.chained_assignment = None\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    " \n",
    " \n",
    "def change_obj_to_category(df):\n",
    "    i=0;\n",
    "    while i<df.columns.size:\n",
    "        if (df.dtypes[df.columns[i]] == 'object' or df.dtypes[df.columns[i]] == 'bool'):\n",
    "            col = df.dtypes.index[i]\n",
    "            df[col] = df[col].astype('category')\n",
    "        i = i+1 \n",
    "        \n",
    "def change_int_to_float(df):\n",
    "    i=0;\n",
    "    while i<df.columns.size:\n",
    "        if 'int' in str(df.dtypes[df.columns[i]]):\n",
    "            col = df.dtypes.index[i]\n",
    "            df[col] = df[col].astype('float64')\n",
    "        i = i+1\n",
    "       \n",
    "def generateXY(df,target_col,var_list):\n",
    "   \n",
    "    #make a copy of the required subset and drop rows containing Na\n",
    "    mdata = df[var_list+[target_col]].copy()\n",
    "    mdata.dropna(inplace=True)\n",
    "   \n",
    "    #Separate target and put it in Y\n",
    "    Y = mdata[target_col].tolist()\n",
    "    del mdata[target_col]\n",
    "   \n",
    "    #Now, process data and create dummy variables if required with final data in Xvars\n",
    "    import pandas\n",
    "    Xvars = pandas.DataFrame()\n",
    "    import pandas as pd\n",
    "    for cols in var_list:\n",
    "        if (str(mdata[cols].dtype) == 'category'):\n",
    "            dummySer= pd.get_dummies(mdata[cols],prefix=cols+'_')\n",
    "            Xvars = pd.concat([Xvars,dummySer],axis=1)\n",
    "        else:\n",
    "            Xvars =  pd.concat([Xvars,mdata[cols]],axis=1)\n",
    "   \n",
    "    X = Xvars.values.tolist()\n",
    "   \n",
    "    return X,Y\n",
    "        \n",
    "        \n",
    "def roc_score_model(model,X,Y):\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    return roc_auc_score(Y,pd.DataFrame((model.predict_proba(X)))[1].tolist())\n",
    " \n",
    "def feature_imp(colNames,imps):\n",
    "    df = pd.DataFrame(columns=('Feature','Importance'),index=[x for x in range(0,len(colNames))])\n",
    "    i = 0\n",
    "    for col in colNames:\n",
    "        df['Feature'][i] = col\n",
    "        df['Importance'][i] = imps[i]\n",
    "        i=i+1\n",
    "   \n",
    "    df = df.sort_values(by='Importance',ascending=False)\n",
    "    return df\n",
    " \n",
    "def predict_th(model,tx,ty,threshold=0.5):\n",
    "    import pandas as pd\n",
    "    probs = model.predict_proba(tx)[:,1].tolist()\n",
    "    predictions = []\n",
    "    for i in range(0,len(ty)):\n",
    "        if probs[i]>threshold:\n",
    "            predictions.append(1)\n",
    "        else:\n",
    "            predictions.append(0)\n",
    "   \n",
    "    return predictions  \n",
    " \n",
    " \n",
    "print('Starting data read from csv file.')\n",
    "match_data = pd.read_csv('/Users/ashokvardhan/Downloads/Divorce_dataset/modeling_dataset.csv',sep=',')\n",
    "cols_to_delete = ['CASEID_NEW','weight1','weight2','weight3','weight4','weight5','weight6','weight7','weight_couples_coresident','members_lt_2','members_13_to_17','members_gt_18','members_2_to_5','members_6_to_12','region_4','region_9','bg_survey_date','next_next_year','intro_summary_codes','target_survey_dt','survey_date']\n",
    "single_valued_columns = ['same_gender_partner','partners_religion_at_16','religion_at_16','has_domestic_partnership','has_civil_union','no_domestic_partnership_or_civil_union','attended_same_college','how_met_online','partner_gender','lived_together_before_married','time_dating_until_married','time_met_until_married','time_met_until_dating','marriage_count_combined','zip_pct_white','zip_pct_black','zip_pct_hispanic','zip_median_income','zip_pct_foreign_born','zip_rural']\n",
    " \n",
    "print('Deleting columns containing huge missing values, Single valued items and id columns which are of no use.')\n",
    "print('No data leakage is present as such.')\n",
    "match_data.drop(cols_to_delete,inplace=True,axis=1)\n",
    "match_data.drop(single_valued_columns,inplace=True,axis=1)\n",
    "match_data.broke_up = match_data.broke_up.astype('category')\n",
    " \n",
    "print('Changing the type of columns from object to category.')\n",
    "change_obj_to_category(match_data)\n",
    " \n",
    "numeric_cols = ['household_size',\n",
    "                 'age',\n",
    "                 'children_in_hh',\n",
    "                 'age_when_relationship_started',\n",
    "                 'time_since_met',\n",
    "                 'time_since_romantic',\n",
    "                 'time_since_cohab',\n",
    "                 'time_since_first_relationship',\n",
    "                 'income']       \n",
    " \n",
    "print('Separating numeric data from the data set to perform numeric imputation on missing values.')\n",
    "numeric_data = match_data[numeric_cols]\n",
    "for col in numeric_cols:\n",
    "    match_data.drop([col],axis=1,inplace=True)\n",
    " \n",
    "match_data = match_data.apply(lambda x:x.fillna(x.value_counts().index[0]))\n",
    "print('Imputed categorical data with the mode of the respective column.')\n",
    "numeric_data = numeric_data.fillna(numeric_data.median())\n",
    "print('Imputed numeric columns to its median value.')\n",
    " \n",
    "#Join numeric data to main data set\n",
    "match_data = match_data.join(numeric_data)\n",
    " \n",
    "print('Setting aside 20% of the data as hold_out.')\n",
    "#Now, let's set 20% of the data as holdout data [test data]\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_data, hold_out = train_test_split(match_data, train_size = 0.8,random_state=2135)\n",
    " \n",
    "print('Using standard scaler of sklearn library to Standardize and scale the numeric inputs.')\n",
    "#Scale the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "train_data[numeric_cols] = scaler.fit_transform(train_data[numeric_cols])\n",
    "hold_out[numeric_cols] = scaler.transform(hold_out[numeric_cols])\n",
    " \n",
    " \n",
    "print('\\nFinding Optimal parameters for 4 different models using GridSearch.')\n",
    "target_col = 'broke_up'\n",
    "varToUse = train_data.columns.tolist()\n",
    "varToUse.remove('broke_up')\n",
    "Train_X,Train_Y = generateXY(train_data,target_col,varToUse)\n",
    "Test_X,Test_Y = generateXY(hold_out,target_col,varToUse)\n",
    " \n",
    " \n",
    "print('Optimal Parameters for: ')\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    " \n",
    "#Regularized Logistic Regression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "sgd = SGDClassifier(random_state=2135)\n",
    "pipeline = Pipeline(steps=[('standardize', preprocessing.StandardScaler())\n",
    "                           , ('model', sgd) ])\n",
    "optimized_sgd = GridSearchCV(estimator=pipeline\n",
    "                            , cv=3\n",
    "                            , param_grid=dict(model__alpha = [0.0001,0.0002,0.0003,0.0004])\n",
    "                            , scoring = 'roc_auc'\n",
    "                            , verbose = 0\n",
    "                           )\n",
    "sgdgc = optimized_sgd.fit(Train_X,Train_Y)\n",
    "print('1. Regularized Logistic Regression: ' + str (sgdgc.best_params_))\n",
    " \n",
    "#Decision Tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier(random_state=2135)\n",
    "pipeline = Pipeline(steps=[('standardize', preprocessing.StandardScaler())\n",
    "                           , ('model', dt) ])\n",
    "optimized_dt = GridSearchCV(estimator=pipeline\n",
    "                            , cv=3\n",
    "                            , param_grid=dict(model__max_depth =  [5,6,7], model__max_features=[20,25,30])\n",
    "                            , scoring = 'roc_auc'\n",
    "                            , verbose = 0\n",
    "                           )\n",
    "dtgc = optimized_dt.fit(Train_X,Train_Y)\n",
    "print('2. Decision Tree: ' + str (dtgc.best_params_))\n",
    " \n",
    "#Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(random_state=2135)\n",
    "pipeline = Pipeline(steps=[('standardize', preprocessing.StandardScaler())\n",
    "                           , ('model', rf) ])\n",
    "optimized_rf = GridSearchCV(estimator=pipeline\n",
    "                            , cv=3\n",
    "                            , param_grid=dict(model__max_depth =  [5,6], model__max_features=[5,10],model__n_estimators=[40,50])\n",
    "                            , scoring = 'roc_auc'\n",
    "                            , verbose = 0\n",
    "                           )\n",
    "rfgc = optimized_rf.fit(Train_X,Train_Y)\n",
    "print('3. Random Forest: ' + str (rfgc.best_params_))\n",
    " \n",
    " \n",
    "#Gradient Boosting Classifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gbc = GradientBoostingClassifier(random_state=2135)\n",
    "pipeline = Pipeline(steps=[('standardize', preprocessing.StandardScaler())\n",
    "                           , ('model', gbc) ])\n",
    "optimized_gbc = GridSearchCV(estimator=pipeline\n",
    "                            , cv=3\n",
    "                            , param_grid=dict(model__max_depth =  [2,3], model__max_features=[20,25],model__n_estimators=[60,70])\n",
    "                            , scoring = 'roc_auc'\n",
    "                            , verbose = 0\n",
    "                           )\n",
    "gbcgc = optimized_gbc.fit(Train_X,Train_Y)\n",
    "print('4. Gradient Boosting Classifier: ' + str (gbcgc.best_params_))\n",
    " \n",
    " \n",
    " \n",
    "print('\\nPerforming Cross validation for 4 different models using optimal parameters obtained from GridSearch.')\n",
    "print('Cross Validation Scores for: ')\n",
    "#\n",
    "best_alpha = float(sgdgc.best_params_['model__alpha'])\n",
    "sgd_cvscore = cross_val_score(SGDClassifier(random_state=2135,alpha=best_alpha),Train_X,Train_Y,cv=3,scoring='roc_auc')\n",
    "print ('1. Regularized Logistic Regression: %f' %np.mean(sgd_cvscore))\n",
    "#\n",
    "best_depth = int(dtgc.best_params_['model__max_depth'])\n",
    "best_features = int(dtgc.best_params_['model__max_features'])\n",
    "dtgc_cvscore = cross_val_score(DecisionTreeClassifier(random_state=2135,max_depth=best_depth,max_features=best_features),Train_X,Train_Y,cv=3,scoring='roc_auc')\n",
    "print ('2. Decision Tree: %f' %np.mean(dtgc_cvscore))\n",
    "#\n",
    "best_depthrf = int(rfgc.best_params_['model__max_depth'])\n",
    "best_featuresrf = int(rfgc.best_params_['model__max_features'])\n",
    "best_estimatorsrf = int(rfgc.best_params_['model__n_estimators'])\n",
    "rfgc_cvscore = cross_val_score(RandomForestClassifier(random_state=2135,n_estimators=best_estimatorsrf,max_depth=best_depthrf,max_features=best_featuresrf),Train_X,Train_Y,cv=3,scoring='roc_auc')\n",
    "print ('3. Random Forest: %f' %np.mean(rfgc_cvscore))\n",
    "#\n",
    "best_depthgbc = int(gbcgc.best_params_['model__max_depth'])\n",
    "best_featuresgbc = int(gbcgc.best_params_['model__max_features'])\n",
    "best_estimatorsgbc = int(gbcgc.best_params_['model__n_estimators'])\n",
    "gbcgc_cvscore = cross_val_score(GradientBoostingClassifier(random_state=2135,n_estimators=best_estimatorsgbc,max_depth=best_depthgbc,max_features=best_featuresgbc),Train_X,Train_Y,cv=3,scoring='roc_auc')\n",
    "print ('4. Gradient Boosting Classifier: %f' %np.mean(gbcgc_cvscore))\n",
    " \n",
    " \n",
    "print ('\\nBased on the Cross validation ouptuts, the best model that can be selected is: \\'Gradient Boosting Classifier\\'')\n",
    " \n",
    "print('\\nFitting Gradient Boosting classifier for the dataset.')\n",
    "print('Testing accuracy of this model on Test data(or hold_out data).')\n",
    "gbc = GradientBoostingClassifier(random_state=2135,n_estimators=best_estimatorsgbc,max_depth=best_depthgbc,max_features=best_featuresgbc)\n",
    "gbc = gbc.fit(Train_X,Train_Y)\n",
    " \n",
    "print('\\nHoldout ROC/AUC accuracy for Gradient Boosting classifier: '+ str(roc_score_model(gbc,Test_X,Test_Y)))\n",
    " \n",
    "#feature_imp(Train_X.colnames,gb.feature_importance_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
